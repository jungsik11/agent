# Model configuration
model_type: "gemma2"
hidden_size: 2048
num_hidden_layers: 24
intermediate_size: 8192
num_attention_heads: 8
num_key_value_heads: 4
rms_norm_eps: 1.0e-6
vocab_size: 32000
head_dim: 256
attn_logit_softmax_capping: 50.0
attn_logit_softcapping: 50.0
final_logit_softmax_capping: 30.0
final_logit_softcapping: 30.0
rope_theta: 10000.0

# Tokenizer path
tokenizer_path: "tokenizer"

# Data configuration
data: data
max_seq_length: 512

# Training parameters
train: true
fine_tune_type: full # 'full' means pre-training from scratch
iters: 2000 # Number of training iterations
batch_size: 2 # Adjust based on your memory
learning_rate: 0.00001 # Stable learning rate
warmup: 200 # Warmup steps to stabilize training early on
grad_clip: 1.0 # Gradient clipping to prevent explosions

# Saving and Logging
adapter_path: models/bitnet_pretrained # Path to save the final trained model
save_every: 10 # Save a checkpoint every 200 iterations
steps_per_report: 10 # Log training loss every 10 iterations
steps_per_eval: 200 # Evaluate on the validation set every 200 iterations

# Seed for reproducibility
seed: 42
