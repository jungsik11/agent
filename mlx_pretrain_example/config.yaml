# Model: We are training from scratch, so we don't specify a base model.
# Instead, we will generate one with `mlx_lm.utils.generate_config` implicitly.
# The trained model will be saved based on the adapter_path.

# Data: Path to the training and validation data.
# The data must be in `.jsonl` format, where each line is a JSON object
# with a "text" key. (e.g. {"text": "..."})
data: data/

# Model: Path to the model directory with a config.json file.
model: scratch_model

# Training parameters
train: true
fine_tune_type: full # 'full' means pre-training from scratch
iters: 100 # Number of training iterations
batch_size: 2
learning_rate: 1e-4
max_seq_length: 256
num_layers: -1 # Use -1 to train all layers in the model

# Saving and Logging
adapter_path: models/pretrained_model # Path to save the final trained model
save_every: 50 # Save a checkpoint every 50 iterations
steps_per_report: 10 # Log training loss every 10 iterations
steps_per_eval: 50 # No validation set, so this is just for checkpoints

# Seed for reproducibility
seed: 42
